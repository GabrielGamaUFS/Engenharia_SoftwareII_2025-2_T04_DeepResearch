# üìò An√°lise de Padr√µes Arquiteturais do DeepResearch com LLMs
## Equipe 1 - DeepResearch
| Nome                              | Matr√≠cula     | Descri√ß√£o da atividade                                                                                   |
|----------------------------------|---------------|-----------------------------------------------------------------------------------------------------------|
| √Ålex Santos Alencar              | 202300061518  | Realiza√ß√£o da an√°lise manual do projeto no GitHub.                                                       |
| Ellen Karolliny dos Santos       | 202300114326  | Defini√ß√£o sobre padr√µes arquiteturais, exemplos sobre os padr√µes mais conhecidos.                             |
| Gabriel Luiz Santos Gama Barreto | 202300114335  | Aux√≠lio na constru√ß√£o do prompt dos modelos e do tutorial. An√°lise dos relat√≥rios gerados pelo deepseek e codellama. |
| Gabriel Ramos de Carvalho        | 202300061920  | Ajuda na escolha dos modelos e cria√ß√£o do prompt. Apresenta√ß√£o e discuss√£o dos resultados obtidos a partir do MistralAI. |
| Jo√£o Andryel Santos Menezes      | 202300061652  | Escolha dos modelos, cria√ß√£o do prompt. An√°lise dos resultados e apresenta√ß√£o do Phi 3 mini.              |
| Larissa Batista dos Santos       | 202300061705  | An√°lise e apresenta√ß√£o dos resultados obtidos utilizando o modelo Qwen2.5.                                        |
| Paloma dos Santos                | 202300061723  | Compara√ß√£o e an√°lise dos modelos selecionados. Ajuda na cria√ß√£o do modelo do documento .docx (Resposta da an√°lise e tutorial). |
| Rauany Ingrid Santos de Jesus    | 202300061760  | Introdu√ß√£o de padr√µes arquiteturais, aux√≠lio na cria√ß√£o do doc de an√°lise, desenvolvimento dos slides e edi√ß√£o do v√≠deo. |

### DeepResearch - https://github.com/Alibaba-NLP/DeepResearch (Modelo que ser√° analisado)
---
## V√≠deo de apresenta√ß√£o dos resultados do projeto
### [Acessar v√≠deo](https://drive.google.com/file/d/1LWLBEVQrYNaxog7Xz0Lq3KmnPvoMZx3o/view?usp=sharing)<br>
---
## üìö Sobre o Tutorial
---
Este tutorial apresenta como objetivo demonstrar o processo, passo a passo, de como identificar os padr√µes arquiteturais do modelo de linguagem DeepResearch a partir de seu reposit√≥rio no github, utilizando e simulando tr√™s grandes modelos de linguagem (LLMs), executados a partir do Google Colab.
---
## üöÄ Op√ß√£o de Atalho (Recomendado)
Caso deseje **abrir diretamente o notebook no Google Colab**, sem seguir as etapas iniciais, utilize o link abaixo:

üîó **Acessar o Notebook no Colab:**  
https://colab.research.google.com/github/GabrielGamaUFS/Engenharia_SoftwareII_2025-2_T04_DeepResearch/blob/main/ESII.ipynb

‚û°Ô∏è **Se utilizar esta op√ß√£o, voc√™ pode seguir diretamente para o passo 2 e logo em seguida para o passo 4 do tutorial.**
---
## üß≠ Tutorial Completo

### 1. Abertura do Ambiente Google Colab (IDE)
Nesta etapa, deve-se acessar o ambiente Google Colab, dispon√≠vel no endere√ßo https://colab.google/, e criar um ‚ÄúNovo Notebook‚Äù ou ‚ÄúNew Notebook‚Äù.

<p align="center"> <img src="assets/tutorial1.jpg" width="70%"> </p>

---

### 2. Prepara√ß√£o do Ambiente

Nesse momento √© importante definir o uso da GPU no Colab. Para isso, acesse o menu ‚ÄúAmbiente de Execu√ß√£o‚Äù na parte superior da p√°gina, em seguida pressione ‚ÄúAlterar o tipo de ambiente de execu√ß√£o‚Äù, selecione a op√ß√£o ‚ÄúGPUs: T4‚Äù e clique em ‚Äúsalvar‚Äù, como segue as figuras:

<p align="center"> <img src="assets/tutorial2.jpg" width="70%"> </p>

<p align="center"> <img src="assets/tutorial3.jpg" width="70%"> </p>

---

### 3. Inser√ß√£o do c√≥digo-fonte
No ambiente do Google Colab, selecione a c√©lula do c√≥digo j√° existente. Caso n√£o haja uma c√©lula j√° criada, pressione a op√ß√£o ‚Äú+  C√≥digo‚Äù para inserir uma nova. 

<p align="center"> <img src="assets/tutorial4.jpg" width="70%"> </p>

Em seguida, cole o trecho do c√≥digo abaixo:

<details>
  <summary><strong>üìå Clique para expandir o c√≥digo completo</strong></summary>

```python
# 1. Instalar as bibliotecas necess√°rias

!pip install transformers accelerate torch bitsandbytes

import os

# Define o diret√≥rio de destino no Colab
repo_dir = "/content/DeepResearch"

# Verifica se a pasta j√° existe antes de clonar
if not os.path.exists(repo_dir):
    print(f"A clonar https://github.com/Alibaba-NLP/DeepResearch para {repo_dir}...")

    !git clone https://github.com/Alibaba-NLP/DeepResearch.git
    print("Reposit√≥rio clonado com sucesso.")
else:
    print(f"Reposit√≥rio j√° existe em {repo_dir}.")

from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
import torch
                                                 # Reinicie a sess√£o sempre que trocar o modelo
# Nome do modelo que voc√™ quer
model_id = "mistralai/Mistral-7B-Instruct-v0.3"  # <--- Alterar pelo modelo escolhido:
                                                 # deepseek-ai/deepseek-coder-6.7b-instruct
print(f"Carregando {model_id} em 4-bit...")      # codellama/CodeLlama-7b-Instruct-hf
                                                 # mistralai/Mistral-7B-Instruct-v0.3
# --- Configura√ß√£o de 4-bit  ---                 # microsoft/Phi-3-mini-128k-instruct
bnb_config = BitsAndBytesConfig(                 # Qwen/Qwen2.5-7B-Instruct
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

# Carregar o tokenizador
tokenizer = AutoTokenizer.from_pretrained(model_id)

# Carregar o modelo aplicando a configura√ß√£o de 4-bit
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    quantization_config=bnb_config,  # <-- Aplicando a configura√ß√£o de 4-bit
    device_map="auto"                # "auto" coloca o modelo na GPU
)

print("----------------------------------------------------------")
print(f"Modelo {model_id} carregado com sucesso em 4-bit!")
print("----------------------------------------------------------")

import os
import subprocess

# --- PASSO 1: COLETAR OS METADADOS ---

repo_path = "/content/DeepResearch"
print(f"Coletando metadados do reposit√≥rio em: {repo_path}")

# 1.1 Coletar Estrutura de Pastas
folder_structure = ""
for root, dirs, files in os.walk(repo_path, topdown=True):
    # Limitar a profundidade da √°rvore (n√≠vel 0, 1 e 2)
    depth = root.replace(repo_path, '').count(os.sep)
    if depth > 2:
        dirs[:] = [] # N√£o explorar mais fundo
        continue

    # Ignorar a pasta .git
    if '.git' in dirs:
        dirs.remove('.git')

    # Formatar o nome da pasta
    indent = "  " * depth
    base_name = os.path.basename(root) if depth > 0 else 'DeepResearch'
    folder_structure += f"{indent}- {base_name}/\n"

print("... Estrutura de Pastas coletada.")

# 1.2 Ler o README.md
try:
    with open(os.path.join(repo_path, "README.md"), 'r', encoding='utf-8') as f:
        readme_content = f.read(3000) + "\n... (README truncado)"
    print("... README.md coletado.")
except Exception as e:
    readme_content = f"Erro ao ler README: {e}"

# 1.3 Ler o requirements.txt
try:
    with open(os.path.join(repo_path, "requirements.txt"), 'r', encoding='utf-8') as f:
        requirements_content = f.read()
    print("... requirements.txt coletado.")
except Exception as e:
    requirements_content = f"Erro ao ler requirements.txt: {e}"

# 1.4 Coletar os logs de commit
try:
    log_command = ["git", "log", "--oneline", "-n", "20"] # 20 commits mais recentes
    result = subprocess.run(log_command, cwd=repo_path, capture_output=True, text=True, check=True, encoding='utf-8')
    git_log_content = result.stdout
    print("... Logs de Commit (√∫ltimos 20) coletados.")
except Exception as e:
    git_log_content = f"Erro ao ler logs do Git: {e}"

# 1.5 Ler o setup.py
try:
    with open(os.path.join(repo_path, "setup.py"), 'r', encoding='utf-8') as f:
        setup_content = f.read(1000) + "\n... (setup.py truncado)"
    print("... setup.py coletado.")
except Exception as e:
    setup_content = "setup.py n√£o encontrado ou erro."

# 1.6 Ler o Ponto de Entrada da API
api_main_path = "WebAgent/WebSailor/main.py"
try:
    with open(os.path.join(repo_path, api_main_path), 'r', encoding='utf-8') as f:
        api_main_content = f.read(2000) + f"\n... ({api_main_path} truncado)"
    print(f"... {api_main_path} coletado.")
except Exception as e:
    api_main_content = f"{api_main_path} n√£o encontrado ou erro."

# --- PASSO 2: CONSTRUIR O PROMPT ARQUITETURAL ---

prompt_arquitetural = f"""
[INST]
Voc√™ √© um Arquiteto de Software S√™nior. Sua tarefa √© analisar os seguintes metadados e **trechos de c√≥digo** de um reposit√≥rio para identificar os Padr√µes Arquiteturais.

Preste muita aten√ß√£o em como os arquivos de c√≥digo (como o `main.py`) **importam e usam** outros m√≥dulos.

---
### DADOS DO REPOSIT√ìRIO PARA AN√ÅLISE ###

#### 1. README.md (Truncado) ####
{readme_content}

#### 2. requirements.txt (Depend√™ncias) ####
{requirements_content}

#### 3. Estrutura de Pastas (N√≠vel 2) ####
{folder_structure}

#### 4. Logs de Commit Recentes ####
{git_log_content}

#### 5. Conte√∫do do setup.py (Define o Pacote) ####
{setup_content}

#### 6. Conte√∫do do Ponto de Entrada da API ({api_main_path}) ####
{api_main_content}

---

### RELAT√ìRIO DE HIP√ìTESE ARQUITETURAL ###

Baseado **apenas** nos dados acima, responda:

Responda **precisamente** √†s seguintes perguntas, baseando-se **apenas** nas evid√™ncias fornecidas:

1Ô∏è‚É£. *Qual √© o Padr√£o de Ponto de Entrada?*
    * Qual tecnologia principal √© usada como interface do sistema?
    * *Justifique* analisando o arquivo {api_main_path} e o requirements.txt.

2Ô∏è‚É£. *Qual √© o Padr√£o de Estrutura de C√≥digo?*
    * *Justifique* sua escolha.

3Ô∏è‚É£. *Qual √© o Padr√£o de Implanta√ß√£o?*
    * Ao analise o c√≥digo do `{api_main_path} e {folder_structure}

4Ô∏è‚É£. *Resumo da Arquitetura:*
    * Combine suas tr√™s respostas acima em uma descri√ß√£o coesa da arquitetura geral.

[/INST]
"""

print("\nPrompt de an√°lise arquitetural constru√≠do e pronto para envio.")

# --- PASSO 3: CHAMAR O LLM ---

print("Enviando prompt de an√°lise arquitetural para o LLM...")

try:

    inputs = tokenizer(prompt_arquitetural, return_tensors="pt", truncation=True, max_length=8192).to("cuda") # 8k para CodeLlama/Mistral

    output = model.generate(
        **inputs,
        max_new_tokens=2024, # Espa√ßo para o relat√≥rio
        pad_token_id=tokenizer.eos_token_id,
        temperature=0.7 # Temperatura mais baixa ajuda a evitar alucina√ß√µes
    )

    response = tokenizer.decode(output[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)

    print("\n\n--- RELAT√ìRIO ARQUITETURAL DO LLM ---")
    print(response)

except NameError as ne:
    print(f"\nErro de Nome: {ne}")
except Exception as e:
    print(f"\nErro ao rodar a an√°lise: {e}")
```
</details>

---

### 4. Execu√ß√£o dos Modelos de Linguagem (LLMs)
Ap√≥s o c√≥digo estar inserido, pressione o bot√£o ‚ÄúExecutar c√©lula‚Äù (√≠cone de play) ou ‚ÄúExecutar tudo‚Äù para processar o modelo e aguarde a conclus√£o da an√°lise realizada pelo LLM. 

<p align="center"> <img src="assets/tutorial5.jpg" width="70%"> </p>

---

### 5. Repeti√ß√£o da simula√ß√£o
O procedimento pode ser repetido para cada um dos cinco modelos de linguagem utilizados na simula√ß√£o, bastando substituir, na linha indicada abaixo, pelo modelo preferido.

<p align="center"> <img src="assets/tutorial6.jpg" width="70%"> </p>

Ap√≥s a substitui√ß√£o do modelo, √© necess√°rio reiniciar a sess√£o do ambiente, Para isso, clique na seta para baixo (‚ÄúMais a√ß√µes‚Äù) e em seguida ‚ÄúReinicar sess√£o‚Äù.
<p align="center"> <img src="assets/tutorial7.jpg" width="70%"> </p>

---

## üìÑ Documenta√ß√£o da an√°lise dos LLMs

Acesse a vers√£o em PDF contendo:

- Introdu√ß√£o ao tema
- Tutorial
- A an√°lise detalhada
- Conclus√µes estruturadas
- Compara√ß√µes entre modelos
- Tabelas <br>
- Dificuldades e limita√ß√µes
- Refer√™ncias

### [Acessar documento](docs/tutorial_e_documentacao_completa.pdf) <br>

